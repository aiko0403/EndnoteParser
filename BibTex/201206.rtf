@ARTICLE{6144060, 
author={Ha, L.K. and Kruger, J. and Comba, J.L.D. and Silva, C.T. and Joshi, S.}, 
journal={Visualization and Computer Graphics, IEEE Transactions on}, 
title={ISP: An Optimal Out-of-Core Image-Set Processing Streaming Architecture for Parallel Heterogeneous Systems}, 
year={2012}, 
month={June}, 
volume={18}, 
number={6}, 
pages={838-851}, 
abstract={Image population analysis is the class of statistical methods that plays a central role in understanding the development, evolution, and disease of a population. However, these techniques often require excessive computational power and memory that are compounded with a large number of volumetric inputs. Restricted access to supercomputing power limits its influence in general research and practical applications. In this paper we introduce ISP, an Image-Set Processing streaming framework that harnesses the processing power of commodity heterogeneous CPU/GPU systems and attempts to solve this computational problem. In ISP, we introduce specially designed streaming algorithms and data structures that provide an optimal solution for out-of-core multiimage processing problems both in terms of memory usage and computational efficiency. ISP makes use of the asynchronous execution mechanism supported by parallel heterogeneous systems to efficiently hide the inherent latency of the processing pipeline of out-of-core approaches. Consequently, with computationally intensive problems, the ISP out-of-core solution can achieve the same performance as the in-core solution. We demonstrate the efficiency of the ISP framework on synthetic and real datasets.}, 
keywords={graphics processing units;image processing;parallel processing;pipeline processing;statistical analysis;CPU-GPU systems;ISP framework;asynchronous execution mechanism;computational efficiency;data structures;image population analysis;memory usage;optimal out-of-core image-set processing streaming architecture;out-of-core approach pipeline processing;parallel heterogeneous systems;statistical methods;streaming algorithms;supercomputing power;Computational modeling;Data models;Graphics processing unit;Hardware;MIMO;Parallel processing;Streaming media;GPUs;atlas construction;diffeomorphism;multiimage processing framework.;out-of-core processing;Algorithms;Brain;Computer Graphics;Databases, Factual;Diagnostic Imaging;Humans;Image Processing, Computer-Assisted;Models, Theoretical;Regression Analysis}, 
doi={10.1109/TVCG.2012.32}, 
ISSN={1077-2626},}
@ARTICLE{6152102, 
author={Biddiscombe, J. and Soumagne, J. and Oger, G. and Guibert, D. and Piccinali, J.-G.}, 
journal={Visualization and Computer Graphics, IEEE Transactions on}, 
title={Parallel Computational Steering for HPC Applications Using HDF5 Files in Distributed Shared Memory}, 
year={2012}, 
month={June}, 
volume={18}, 
number={6}, 
pages={852-864}, 
abstract={Interfacing a GUI driven visualization/analysis package to an HPC application enables a supercomputer to be used as an interactive instrument. We achieve this by replacing the IO layer in the HDF5 library with a custom driver which transfers data in parallel between simulation and analysis. Our implementation using ParaView as the interface, allows a flexible combination of parallel simulation, concurrent parallel analysis, and GUI client, either on the same or separate machines. Each MPI job may use different core counts or hardware configurations, allowing fine tuning of the amount of resources dedicated to each part of the workload. By making use of a distributed shared memory file, one may read data from the simulation, modify it using ParaView pipelines, write it back, to be reused by the simulation (or vice versa). This allows not only simple parameter changes, but complete remeshing of grids, or operations involving regeneration of field values over the entire domain. To avoid the problem of manually customizing the GUI for each application that is to be steered, we make use of XML templates that describe outputs from the simulation (and inputs back to it) to automatically generate GUI controls for manipulation of the simulation.}, 
keywords={XML;application program interfaces;data analysis;data visualisation;device drivers;distributed shared memory systems;graphical user interfaces;interactive systems;mesh generation;parallel processing;pipeline processing;GUI client;GUI driven visualization-analysis package;HDF5 files;HDF5 library;HPC application;IO layer;MPI job;ParaView pipelines;XML templates;concurrent parallel analysis;custom driver;distributed shared memory file;grid remeshing;hardware configurations;interactive instrument;parallel computational steering;parallel simulation;supercomputer;Analytical models;Computational modeling;Data models;Graphical user interfaces;Libraries;Servers;Synchronization;Parallel I/O;distributed/network graphics;software libraries.}, 
doi={10.1109/TVCG.2012.63}, 
ISSN={1077-2626},}
@ARTICLE{5928344, 
author={Nieser, M. and Palacios, J. and Polthier, K. and Zhang, E.}, 
journal={Visualization and Computer Graphics, IEEE Transactions on}, 
title={Hexagonal Global Parameterization of Arbitrary Surfaces}, 
year={2012}, 
month={June}, 
volume={18}, 
number={6}, 
pages={865-878}, 
abstract={We introduce hexagonal global parameterization, a new type of surface parameterization in which parameter lines respect sixfold rotational symmetries (6-RoSy). Such parameterizations enable the tiling of surfaces with nearly regular hexagonal or triangular patterns, and can be used for triangular remeshing. Our framework to construct a hexagonal parameterization, referred to as HEXCOVER, extends the QUADCOVER algorithm and formulates necessary conditions for hexagonal parameterization. We also provide an algorithm to automatically generate a 6-RoSy field that respects directional and singularity features in the surface. We demonstrate the usefulness of our geometry-aware global parameterization with applications such as surface tiling with nearly regular textures and geometry patterns, as well as triangular and hexagonal remeshing.}, 
keywords={computational geometry;image texture;mesh generation;solid modelling;HEXCOVER;QUADCOVER algorithm;arbitrary surfaces;geometry patterns;geometry-aware global parameterization;hexagonal global parameterization;hexagonal remeshing;parameter lines;regular textures;sixfold rotational symmetries;surface parameterization;triangular remeshing;Clustering algorithms;Electronic mail;Geometry;Indexes;Pipelines;Surface texture;Tensile stress;Surface parameterization;geometry synthesis;hexagonal global parameterization;pattern synthesis on surfaces;regular patterns.;rotational symmetry;texture synthesis;triangular remeshing}, 
doi={10.1109/TVCG.2011.118}, 
ISSN={1077-2626},}
@ARTICLE{5928345, 
author={Shi-Qing Xin and Ying He and Chi-Wing Fu}, 
journal={Visualization and Computer Graphics, IEEE Transactions on}, 
title={Efficiently Computing Exact Geodesic Loops within Finite Steps}, 
year={2012}, 
month={June}, 
volume={18}, 
number={6}, 
pages={879-889}, 
abstract={Closed geodesics, or geodesic loops, are crucial to the study of differential topology and differential geometry. Although the existence and properties of closed geodesics on smooth surfaces have been widely studied in mathematics community, relatively little progress has been made on how to compute them on polygonal surfaces. Most existing algorithms simply consider the mesh as a graph and so the resultant loops are restricted only on mesh edges, which are far from the actual geodesics. This paper is the first to prove the existence and uniqueness of geodesic loop restricted on a closed face sequence; it contributes also with an efficient algorithm to iteratively evolve an initial closed path on a given mesh into an exact geodesic loop within finite steps. Our proposed algorithm takes only an O(k) space complexity and an O(mk) time complexity (experimentally), where m is the number of vertices in the region bounded by the initial loop and the resultant geodesic loop, and k is the average number of edges in the edge sequences that the evolving loop passes through. In contrast to the existing geodesic curvature flow methods which compute an approximate geodesic loop within a predefined threshold, our method is exact and can apply directly to triangular meshes without needing to solve any differential equation with a numerical solver; it can run at interactive speed, e.g., in the order of milliseconds, for a mesh with around 50K vertices, and hence, significantly outperforms existing algorithms. Actually, our algorithm could run at interactive speed even for larger meshes. Besides the complexity of the input mesh, the geometric shape could also affect the number of evolving steps, i.e., the performance. We motivate our algorithm with an interactive shape segmentation example shown later in the paper.}, 
keywords={computational complexity;computational geometry;differential geometry;image segmentation;mesh generation;closed face sequence;closed path;differential geometry;differential topology;edge sequences;exact geodesic loop;finite steps;geodesic curvature flow method;geometric shape;interactive shape segmentation;interactive speed;mathematics community;polygonal surfaces;space complexity;time complexity;triangular mesh edge;Approximation algorithms;Approximation methods;Complexity theory;Face;Image edge detection;Measurement;Shape;Discrete geodesic;geodesic loop;triangular mesh.}, 
doi={10.1109/TVCG.2011.119}, 
ISSN={1077-2626},}
@ARTICLE{5928342, 
author={Wilkie, D. and Sewall, J. and Lin, M.C.}, 
journal={Visualization and Computer Graphics, IEEE Transactions on}, 
title={Transforming GIS Data into Functional Road Models for Large-Scale Traffic Simulation}, 
year={2012}, 
month={June}, 
volume={18}, 
number={6}, 
pages={890-901}, 
abstract={There exists a vast amount of geographic information system (GIS) data that model road networks around the world as polylines with attributes. In this form, the data are insufficient for applications such as simulation and 3D visualization-tools which will grow in power and demand as sensor data become more pervasive and as governments try to optimize their existing physical infrastructure. In this paper, we propose an efficient method for enhancing a road map from a GIS database to create a geometrically and topologically consistent 3D model to be used in real-time traffic simulation, interactive visualization of virtual worlds, and autonomous vehicle navigation. The resulting representation provides important road features for traffic simulations, including ramps, highways, overpasses, legal merge zones, and intersections with arbitrary states, and it is independent of the simulation methodologies. We test the 3D models of road networks generated by our algorithm on real-time traffic simulation using both macroscopic and microscopic techniques.}, 
keywords={computational geometry;data visualisation;digital simulation;geographic information systems;interactive systems;solid modelling;traffic engineering computing;virtual reality;3D visualization;GIS database;arbitrary states;autonomous vehicle navigation;functional road models;geographic information system data;geometrically consistent 3D model;highways;interactive visualization;intersections;large-scale traffic simulation;legal merge zones;macroscopic techniques;microscopic techniques;overpasses;polylines;ramps;sensor data;topologically consistent 3D model;virtual worlds;Computational modeling;Data models;Data visualization;Geographic information systems;Geometry;Roads;Virtual world;geometric modeling.}, 
doi={10.1109/TVCG.2011.116}, 
ISSN={1077-2626},}
@ARTICLE{5928338, 
author={Chih-Yuan Yao and Ming-Te Chi and Tong-Yee Lee and Tao Ju}, 
journal={Visualization and Computer Graphics, IEEE Transactions on}, 
title={Region-Based Line Field Design Using Harmonic Functions}, 
year={2012}, 
month={June}, 
volume={18}, 
number={6}, 
pages={902-913}, 
abstract={Field design has wide applications in graphics and visualization. One of the main challenges in field design has been how to provide users with both intuitive control over the directions in the field on one hand and robust management of its topology on the other hand. In this paper, we present a design paradigm for line fields that addresses this challenge. Rather than asking users to input all singularities as in most methods that offer topology control, we let the user provide a partitioning of the domain and specify simple flow patterns within the partitions. Represented by a selected set of harmonic functions, the elementary fields within the partitions are then combined to form continuous fields with rich appearances and well-determined topology. Our method allows a user to conveniently design the flow patterns while having precise and robust control over the topological structure. Based on the method, we developed an interactive tool for designing line fields from images, and demonstrated the utility of the fields in image stylization.}, 
keywords={data visualisation;harmonic analysis;topology;continuous fields;design paradigm;flow patterns;graphics;harmonic functions;image stylization;intuitive control;region-based line field design;robust topology management;topology control;visualization;Algorithm design and analysis;Harmonic analysis;Indexes;Robustness;Skeleton;Surface treatment;Topology;Field design;harmonic functions.;line field;singularity}, 
doi={10.1109/TVCG.2011.112}, 
ISSN={1077-2626},}
@ARTICLE{6143943, 
author={Corsini, M. and Cignoni, P. and Scopigno, R.}, 
journal={Visualization and Computer Graphics, IEEE Transactions on}, 
title={Efficient and Flexible Sampling with Blue Noise Properties of Triangular Meshes}, 
year={2012}, 
month={June}, 
volume={18}, 
number={6}, 
pages={914-924}, 
abstract={This paper deals with the problem of taking random samples over the surface of a 3D mesh describing and evaluating efficient algorithms for generating different distributions. We discuss first the problem of generating a Monte Carlo distribution in an efficient and practical way avoiding common pitfalls. Then, we propose Constrained Poisson-disk sampling, a new Poisson-disk sampling scheme for polygonal meshes which can be easily tweaked in order to generate customized set of points such as importance sampling or distributions with generic geometric constraints. In particular, two algorithms based on this approach are presented. An in-depth analysis of the frequency characterization and performance of the proposed algorithms are also presented and discussed.}, 
keywords={Monte Carlo methods;Poisson distribution;computational geometry;mesh generation;sampling methods;solid modelling;3D mesh surface;Monte Carlo distribution;blue noise properties;constrained Poisson-disk sampling scheme;flexible sampling;frequency characterization;generic geometric constraints;importance sampling;in-depth analysis;polygonal meshes;triangular meshes;Algorithm design and analysis;Complexity theory;Context;Monte Carlo methods;Noise;Three dimensional displays;Tiles;Geometry processing;Monte Carlo methods.;Poisson-disk sampling;computational geometry;sampling;three-dimensional graphics and realism}, 
doi={10.1109/TVCG.2012.34}, 
ISSN={1077-2626},}
@ARTICLE{5928339, 
author={Fei Yang and Qingde Li and Dehui Xiang and Yong Cao and Tian, Jie}, 
journal={Visualization and Computer Graphics, IEEE Transactions on}, 
title={A Versatile Optical Model for Hybrid Rendering of Volume Data}, 
year={2012}, 
month={June}, 
volume={18}, 
number={6}, 
pages={925-937}, 
abstract={In volume rendering, most optical models currently in use are based on the assumptions that a volumetric object is a collection of particles and that the macro behavior of particles, when they interact with light rays, can be predicted based on the behavior of each individual particle. However, such models are not capable of characterizing the collective optical effect of a collection of particles which dominates the appearance of the boundaries of dense objects. In this paper, we propose a generalized optical model that combines particle elements and surface elements together to characterize both the behavior of individual particles and the collective effect of particles. The framework based on a new model provides a more powerful and flexible tool for hybrid rendering of isosurfaces and transparent clouds of particles in a single scene. It also provides a more rational basis for shading, so the problem of normal-based shading in homogeneous regions encountered in conventional volume rendering can be easily avoided. The model can be seen as an extension to the classical model. It can be implemented easily, and most of the advanced numerical estimation methods previously developed specifically for the particle-based optical model, such as preintegration, can be applied to the new model to achieve high-quality rendering results.}, 
keywords={computational geometry;rendering (computer graphics);dense objects;hybrid volume data rendering;isosurfaces;light rays;normal-based shading;numerical estimation methods;particle elements;particle macro behavior;particle-based optical model;preintegration;surface elements;transparent particle clouds;versatile optical model;volumetric object;Equations;Isosurfaces;Mathematical model;Numerical models;Rendering (computer graphics);Solid modeling;Transfer functions;Direct volume rendering;isosurfaces;optical models;preintegration;ray casting;transfer function.}, 
doi={10.1109/TVCG.2011.113}, 
ISSN={1077-2626},}
@ARTICLE{6143903, 
author={Szymczak, A. and Zhang, E.}, 
journal={Visualization and Computer Graphics, IEEE Transactions on}, 
title={Robust Morse Decompositions of Piecewise Constant Vector Fields}, 
year={2012}, 
month={June}, 
volume={18}, 
number={6}, 
pages={938-951}, 
abstract={In this paper, we introduce a new approach to computing a Morse decomposition of a vector field on a triangulated manifold surface. The basic idea is to convert the input vector field to a piecewise constant (PC) vector field, whose trajectories can be computed using simple geometric rules. To overcome the intrinsic difficulty in PC vector fields (in particular, discontinuity along mesh edges), we borrow results from the theory of differential inclusions. The input vector field and its PC variant have similar Morse decompositions. We introduce a robust and efficient algorithm to compute Morse decompositions of a PC vector field. Our approach provides subtriangle precision for Morse sets. In addition, we describe a Morse set classification framework which we use to color code the Morse sets in order to enhance the visualization. We demonstrate the benefits of our approach with three well-known simulation data sets, for which our method has produced Morse decompositions that are similar to or finer than those obtained using existing techniques, and is over an order of magnitude faster.}, 
keywords={computational geometry;data visualisation;mesh generation;pattern classification;differential inclusions;geometric rules;mesh discontinuity;morse set classification framework;piecewise constant vector field;robust morse decompositions;simulation data sets;subtriangle precision;triangulated manifold surface;visualization enhancement;Indexes;Orbits;Spirals;Support vector machine classification;Topology;Trajectory;Vectors;Morse decomposition;vector field topology.}, 
doi={10.1109/TVCG.2011.88}, 
ISSN={1077-2626},}
@ARTICLE{5928335, 
author={Etiene, T. and Nonato, L.G. and Scheidegger, C. and Tienry, J. and Peters, T.J. and Pascucci, V. and Kirby, R.M. and Silva, C.T.}, 
journal={Visualization and Computer Graphics, IEEE Transactions on}, 
title={Topology Verification for Isosurface Extraction}, 
year={2012}, 
month={June}, 
volume={18}, 
number={6}, 
pages={952-965}, 
abstract={The broad goals of verifiable visualization rely on correct algorithmic implementations. We extend a framework for verification of isosurfacing implementations to check topological properties. Specifically, we use stratified Morse theory and digital topology to design algorithms which verify topological invariants. Our extended framework reveals unexpected behavior and coding mistakes in popular publicly available isosurface codes.}, 
keywords={computational geometry;data visualisation;topology;algorithmic implementations;digital topology;isosurface coding mistakes;isosurface extraction;stratified Morse theory;topological invariant verification;topology verification;verifiable visualization;Face;Interpolation;Isosurfaces;Level set;Manifolds;Software;Topology;Verifiable visualization;isosurface;topology.}, 
doi={10.1109/TVCG.2011.109}, 
ISSN={1077-2626},}
@ARTICLE{5753896, 
author={Krishnan, H. and Garth, C. and Guhring, J. and Gulsun, M.A. and Greiser, A. and Joy, K.I.}, 
journal={Visualization and Computer Graphics, IEEE Transactions on}, 
title={Analysis of Time-Dependent Flow-Sensitive PC-MRI Data}, 
year={2012}, 
month={June}, 
volume={18}, 
number={6}, 
pages={966-977}, 
abstract={Many flow visualization techniques, especially integration-based methods, are problematic when the measured data exhibit noise and discretization issues. Particularly, this is the case for flow-sensitive phase-contrast magnetic resonance imaging (PC-MRI) data sets which not only record anatomic information, but also time-varying flow information. We propose a novel approach for the visualization of such data sets using integration-based methods. Our ideas are based upon finite-time Lyapunov exponents (FTLE) and enable identification of vessel boundaries in the data as high regions of separation. This allows us to correctly restrict integration-based visualization to blood vessels. We validate our technique by comparing our approach to existing anatomy-based methods as well as addressing the benefits and limitations of using FTLE to restrict flow. We also discuss the importance of parameters, i.e., advection length and data resolution, in establishing a well-defined vessel boundary. We extract appropriate flow lines and surfaces that enable the visualization of blood flow within the vessels. We further enhance the visualization by analyzing flow behavior in the seeded region and generating simplified depictions.}, 
keywords={biomedical MRI;blood vessels;data visualisation;flow visualisation;haemodynamics;integration;medical computing;MRI;advection length;anatomic information;blood flow visualization techniques;blood vessel boundary identification;data resolution;finite-time Lyapunov exponents;flow line extraction;flow-sensitive phase-contrast magnetic resonance imaging data sets;integration-based visualization methods;surface extraction;time-varying flow information;Biomedical imaging;Blood;Data visualization;Magnetic resonance imaging;Trajectory;Visualization;Flow analysis;flow-sensitive MRI;medical visualization.;surface extraction;time-varying and time-series visualization;Algorithms;Blood Flow Velocity;Computer Graphics;Humans;Image Processing, Computer-Assisted;Magnetic Resonance Angiography;Models, Cardiovascular;Thorax}, 
doi={10.1109/TVCG.2011.80}, 
ISSN={1077-2626},}
@ARTICLE{5753898, 
author={Jianu, R. and Demiralp, C. and Laidlaw, D.H.}, 
journal={Visualization and Computer Graphics, IEEE Transactions on}, 
title={Exploring Brain Connectivity with Two-Dimensional Neural Maps}, 
year={2012}, 
month={June}, 
volume={18}, 
number={6}, 
pages={978-987}, 
abstract={We introduce two-dimensional neural maps for exploring connectivity in the brain. For this, we create standard streamtube models from diffusion-weighted brain imaging data sets along with neural paths hierarchically projected into the plane. These planar neural maps combine desirable properties of low-dimensional representations, such as visual clarity and ease of tract-of-interest selection, with the anatomical familiarity of 3D brain models and planar sectional views. We distribute this type of visualization both in a traditional stand-alone interactive application and as a novel, lightweight web-accessible system. The web interface integrates precomputed neural-path representations into a geographical digital-maps framework with associated labels, metrics, statistics, and linkouts. Anecdotal and quantitative comparisons of the present method with a recently proposed 2D point representation suggest that our representation is more intuitive and easier to use and learn. Similarly, users are faster and more accurate in selecting bundles using the 2D path representation than the 2D point representation. Finally, expert feedback on the web interface suggests that it can be useful for collaboration as well as quick exploration of data.}, 
keywords={Internet;biodiffusion;biomedical MRI;brain models;brain-computer interfaces;data visualisation;interactive systems;medical computing;neural nets;neurophysiology;3D brain models;Web interface;anecdotal study;brain connectivity;diffusion weighted brain imaging data sets;geographical digital maps framework;lightweight Web accessible system;low-dimensional representations;planar neural maps;planar sectional views;quantitative study;stand-alone interactive application;standard streamtube models;tract-of-interest selection;two-dimensional neural maps;visual clarity;visualization;Computational modeling;Data visualization;Diffusion tensor imaging;Rendering (computer graphics);Splines (mathematics);Three dimensional displays;Visualization;DTI fiber tracts;abstraction;coloring.;filtration;interaction;path immersion;Algorithms;Brain;Brain Mapping;Cluster Analysis;Computer Graphics;Diffusion Tensor Imaging;Female;Humans;Image Processing, Computer-Assisted;Male;Nerve Fibers;Neural Pathways}, 
doi={10.1109/TVCG.2011.82}, 
ISSN={1077-2626},}
@ARTICLE{5887327, 
author={Angus, D. and Smith, A. and Wiles, J.}, 
journal={Visualization and Computer Graphics, IEEE Transactions on}, 
title={Conceptual Recurrence Plots: Revealing Patterns in Human Discourse}, 
year={2012}, 
month={June}, 
volume={18}, 
number={6}, 
pages={988-997}, 
abstract={Human discourse contains a rich mixture of conceptual information. Visualization of the global and local patterns within this data stream is a complex and challenging problem. Recurrence plots are an information visualization technique that can reveal trends and features in complex time series data. The recurrence plot technique works by measuring the similarity of points in a time series to all other points in the same time series and plotting the results in two dimensions. Previous studies have applied recurrence plotting techniques to textual data; however, these approaches plot recurrence using term-based similarity rather than conceptual similarity of the text. We introduce conceptual recurrence plots, which use a model of language to measure similarity between pairs of text utterances, and the similarity of all utterances is measured and displayed. In this paper, we explore how the descriptive power of the recurrence plotting technique can be used to discover patterns of interaction across a series of conversation transcripts. The results suggest that the conceptual recurrence plotting technique is a useful tool for exploring the structure of human discourse.}, 
keywords={data visualisation;text analysis;time series;complex time series data;conceptual information;conceptual recurrence plot technique;conversation transcripts;data stream;global-local pattern visualization;human discourse;information visualization technique;interaction pattern discovery;language modelling;pattern revealing;point similarity measurement;term-based similarity;text utterances;Data visualization;Humans;Image color analysis;Pain;Semantics;Surgery;Time series analysis;Concept map;concept;conversation analysis;plotting;recurrence;text analysis.;Communication;Computer Graphics;Databases, Factual;Humans;Narration;Natural Language Processing;Pattern Recognition, Automated;Speech}, 
doi={10.1109/TVCG.2011.100}, 
ISSN={1077-2626},}
@ARTICLE{5930386, 
author={Streit, M. and Schulz, H. and Lex, A. and Schmalstieg, D. and Schumann, H.}, 
journal={Visualization and Computer Graphics, IEEE Transactions on}, 
title={Model-Driven Design for the Visual Analysis of Heterogeneous Data}, 
year={2012}, 
month={June}, 
volume={18}, 
number={6}, 
pages={998-1010}, 
abstract={As heterogeneous data from different sources are being increasingly linked, it becomes difficult for users to understand how the data are connected, to identify what means are suitable to analyze a given data set, or to find out how to proceed for a given analysis task. We target this challenge with a new model-driven design process that effectively codesigns aspects of data, view, analytics, and tasks. We achieve this by using the workflow of the analysis task as a trajectory through data, interactive views, and analytical processes. The benefits for the analysis session go well beyond the pure selection of appropriate data sets and range from providing orientation or even guidance along a preferred analysis path to a potential overall speedup, allowing data to be fetched ahead of time. We illustrate the design process for a biomedical use case that aims at determining a treatment plan for cancer patients from the visual analysis of a large, heterogeneous clinical data pool. As an example for how to apply the comprehensive design approach, we present Stack'n'flip, a sample implementation which tightly integrates visualizations of the actual data with a map of available data sets, views, and tasks, thus capturing and communicating the analytical workflow through the required data sets.}, 
keywords={cancer;data visualisation;medical computing;patient treatment;Stacknflip;analysis task workflow;analytical process;biomedical use case;cancer patient treatment plan;data trajectory;data visualization;heterogeneous clinical data pool visual analysis;interactive views;model-driven design process;Analytical models;Biological system modeling;Computational modeling;Concrete;Data models;Data visualization;Visualization;Visual analytics;analysis guidance;model-driven design;multiple data sets.;Cluster Analysis;Computational Biology;Computer Graphics;Databases, Factual;Humans;Medical Informatics Applications;Models, Theoretical;Neoplasms}, 
doi={10.1109/TVCG.2011.108}, 
ISSN={1077-2626},}